{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fusion Model Training (XGBoost)\n",
    "\n",
    "Train XGBoost on fused features: `[tabular features || graph embeddings]`\n",
    "\n",
    "**Protocol A (default):** Local features (AF1-93) + embeddings\n",
    "\n",
    "**Steps:**\n",
    "1. Load embeddings and features\n",
    "2. Merge on txId\n",
    "3. Train XGBoost with early stopping\n",
    "4. Evaluate and compare with baseline metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "from src.data.elliptic_loader import EllipticDataset\n",
    "from src.data.merge_embeddings import merge_embeddings_with_features\n",
    "from src.train.fusion_xgb import train_xgb_fusion\n",
    "from src.eval.fusion_report import create_comparison_report\n",
    "from src.utils.seed import set_all_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "with open('../configs/fusion_xgb.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "set_all_seeds(config['seed'])\n",
    "print(f\"Config loaded: {config['experiment']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_root = Path(config['data']['root'])\n",
    "dataset = EllipticDataset(data_root, use_local_only=config['data']['use_local_only'])\n",
    "print(f\"Dataset loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge embeddings with features\n",
    "fused_df = merge_embeddings_with_features(\n",
    "    embeddings_path=config['embed']['save_path'],\n",
    "    features_path=data_root / config['data']['features'],\n",
    "    use_local_only=config['data']['use_local_only']\n",
    ")\n",
    "\n",
    "print(f\"Fused dataset shape: {fused_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train/val/test splits\n",
    "# Align with dataset splits\n",
    "train_mask = dataset.splits['train']\n",
    "val_mask = dataset.splits['val']\n",
    "test_mask = dataset.splits['test']\n",
    "\n",
    "# Get labeled nodes only\n",
    "train_labeled = dataset.get_labeled_mask(train_mask)\n",
    "val_labeled = dataset.get_labeled_mask(val_mask)\n",
    "test_labeled = dataset.get_labeled_mask(test_mask)\n",
    "\n",
    "# Extract features (exclude txId, Time step)\n",
    "feature_cols = [col for col in fused_df.columns if col not in ['txId', 'Time step']]\n",
    "\n",
    "X_train = fused_df.loc[train_labeled, feature_cols].values\n",
    "y_train = dataset.labels[train_labeled]\n",
    "\n",
    "X_val = fused_df.loc[val_labeled, feature_cols].values\n",
    "y_val = dataset.labels[val_labeled]\n",
    "\n",
    "X_test = fused_df.loc[test_labeled, feature_cols].values\n",
    "y_test = dataset.labels[test_labeled]\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost\n",
    "model, metrics = train_xgb_fusion(\n",
    "    X_train, y_train,\n",
    "    X_val, y_val,\n",
    "    X_test, y_test,\n",
    "    config=config['fusion']['xgb'],\n",
    "    output_dir=config['logging']['out_dir']\n",
    ")\n",
    "\n",
    "print(\"\\n=== Test Metrics ===\")\n",
    "for k, v in metrics['test'].items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comparison report with baseline\n",
    "comparison_df = create_comparison_report(\n",
    "    fusion_metrics=metrics,\n",
    "    baseline_csv=config['baseline']['metrics_csv'],\n",
    "    output_dir=config['logging']['out_dir']\n",
    ")\n",
    "\n",
    "print(\"\\n=== Model Comparison (Test Set) ===\")\n",
    "test_comparison = comparison_df[comparison_df['split'] == 'test'][['model', 'pr_auc', 'roc_auc', 'f1']]\n",
    "print(test_comparison.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
